%%%%%%%%%%%%%%%%%%%%%%  Chapter 1 - web

%%%%%%%%%%%%%%%%%%%%%%  Chapter 1 - article

%%%%%%%%%%%%%%%%%%%%%%  Chapter 2 - web
@online{papers_image,
mendeley-groups = {Capitulo 2 - web},
author = {Paperswithcode},
title = {{ImageNet Leaderboard | Papers with Code}},
url = {https://paperswithcode.com/sota/image-classification-on-imagenet},
urldate = {2020-03-06}
}


@online{cnnarchitectures,
mendeley-groups = {Capitulo 2 - web},
author = {Raimi Karim},
title = {{Illustrated: 10 CNN Architectures}},
url = {https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d},
urldate = {2020-03-12}
}

@online{papers_object,
mendeley-groups = {Capitulo 2 - web},
author = {Paperswithcode},
title = {{COCO test-dev Leaderboard | Papers with Code}},
url = {https://paperswithcode.com/sota/object-detection-on-coco},
urldate = {2020-03-06}
}

@online{mathworks_NN,
mendeley-groups = {Capitulo 2 - web},
author = {MathWorks},
title = {{What Is a Neural Network? - MATLAB {\&} Simulink}},
url = {https://www.mathworks.com/discovery/neural-network.html},
urldate = {2020-03-06}
}

@online{mathworks_ML,
author = {MathWorks},
title = {{What Is a Machine Learning? - MATLAB {\&} Simulink}},
url = {https://www.mathworks.com/discovery/machine-learning.html},
urldate = {2020-03-12}
}



@online{mathworks_AI,
mendeley-groups = {Capitulo 2 - web},
author = {MathWorks},
title = {{What Is Artificial Intelligence? | KurzweilAI}},
url = {https://www.mathworks.com/discovery/artificial-intelligence.html http://www.kurzweilai.net/what-is-artificial-intelligence},
urldate = {2020-03-06}
}


@online{neural_image,
mendeley-groups = {Capitulo 2 - web},
author = {Kjell Magne Fauske},
title = {{What Is Deep Learning? | How It Works, Techniques {\&} Applications - MATLAB {\&} Simulink}},
url = {https://www.mathworks.com/discovery/deep-learning.html},
urldate = {2020-03-05},
year = {2019}
}


@online{mathworks_deeplearning,
mendeley-groups = {Capitulo 2 - web},
author = {MathWorks},
title = {{What Is Deep Learning? | How It Works, Techniques {\&} Applications - MATLAB {\&} Simulink}},
url = {https://www.mathworks.com/discovery/deep-learning.html},
urldate = {2020-03-05},
year = {2019}
}

}

@online{feature,
mendeley-groups = {Capitulo 2 - web},
title = {{The Computer Vision Pipeline, Part 4: feature extraction | Manning}},
url = {https://freecontent.manning.com/the-computer-vision-pipeline-part-4-feature-extraction/},
urldate = {2020-02-18}
}

@online{cv,
author = {Adrien Kaiser},
mendeley-groups = {Capitulo 2 - web},
title = {{What is Computer Vision? | Hayo}},
url = {https://hayo.io/computer-vision/},
urldate = {2020-01-22},
}

@online{cv2,
author = {Jason Brownlee},
mendeley-groups = {Capitulo 2 - web},
title = {{A Gentle Introduction to Computer Vision}},
url = {https://machinelearningmastery.com/what-is-computer-vision/},
urldate = {2020-01-22},

}

@online{pattern,
author = {Ilija Mihajlovic},
mendeley-groups = {Capitulo 2 - web},
title = {{Everything You Ever Wanted To Know About Computer Vision.}},
url = {https://towardsdatascience.com/everything-you-ever-wanted-to-know-about-computer-vision-heres-a-look-why-it-s-so-awesome-e8a58dfb641e},
urldate = {2020-01-22},

}

@online{ObjectDetection,
author = {Jason Brownlee},
mendeley-groups = {Capitulo 2 - web},
title = {{A Gentle Introduction to Object Recognition With Deep Learning}},
url = {https://machinelearningmastery.com/object-recognition-with-deep-learning/},
urldate = {2020-01-22},

}


@online{annotation,
author = {Limarc Ambalina},
mendeley-groups = {Capitulo 2 - web},
title = {{What is Image Annotation? – An Intro to 5 Image Annotation Services - By Limarc Ambalina}},
url = {https://hackernoon.com/what-is-image-annotation-an-intro-to-5-image-annotation-services-yt6n3xfj},
urldate = {2020-01-22}
}


@online{opencvweb,
author = {OpenCV Team},
mendeley-groups = {Capitulo 2 - web},
title = {{About}},
url = {https://opencv.org/about/},
urldate = {2020-01-23}

}

%VLFEAT
@online{vedaldi08vlfeat,
 Author = {A. Vedaldi and B. Fulkerson},
 Title = {{VLFeat}: An Open and Portable Library
          of Computer Vision Algorithms},
 Year  = {2008},
 Howpublished = {\url{http://www.vlfeat.org/}},
 urldate = {2020-01-23}
}

@online{boofcvweb,
Author = {BoofCV Team},
mendeley-groups = {Capitulo 2 - web},
title = {{BoofCV}},
url = {https://boofcv.org/index.php?title=Main{\_}Page},
urldate = {2020-01-23}
}


@online{inceptionV3web,
author = {Sik-Ho Tsang},
mendeley-groups = {Capitulo 2 - web},
title = {{Review: Inception-v3 — 1st Runner Up (Image Classification) in ILSVRC 2015}},
url = {https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c},
urldate = {2020-01-23}
}

@online{inceptionV3web2,
author = {Bharath Raj},
mendeley-groups = {Capitulo 2 - web},
title = {{A Simple Guide to the Versions of the Inception Network}},
url = {https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202},
urldate = {2020-01-23}
}
@article{weng2018detection4,
  title   = "Object Detection Part 4: Fast Detection Models",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io/lil-log",
  year    = "2018",
  url     = "http://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html"
}

@online{ArmaanMerchant2018,
author = {{Armaan Merchant}},
mendeley-groups = {Capitulo 2 - web},
title = {{Neural Networks Explained – Data Driven Investor – Medium}},
url = {https://medium.com/datadriveninvestor/neural-networks-explained-6e21c70d7818},
urldate = {2020-03-04},
year = {2018}
}

@online{VikasGupta2017,
abstract = {In this article, we will learn about feedforward Neural Networks, also known as Deep feedforward Networks or Multi-layer Perceptrons. They form the basis of many important Neural Networks being used in the recent times, such as Convolutional Neural Networks ( used extensively in computer vision applications ), Recurrent Neural Networks ( widely used in Natural language understanding and sequence learning) and so on. We will try to understand the important concepts involved in an intuitive and interactive way, without going into the mathematics involved. If you are interested in diving into deep learning but don't have much background in statistics and machine learning, then this article is a perfect starting point.},
author = {{Vikas Gupta}},
journal = {Learn Open CV},
mendeley-groups = {Capitulo 2 - web},
pages = {1},
title = {{Understanding Feedforward Neural Networks}},
url = {https://www.learnopencv.com/understanding-feedforward-neural-networks/},
year = {2017}
}

%%%%%%%%%%%%%%%%%%%%%%  Chapter 2 - article - tabelas
@misc{tan2018mnasnet,
    title={MnasNet: Platform-Aware Neural Architecture Search for Mobile},
    author={Mingxing Tan and Bo Chen and Ruoming Pang and Vijay Vasudevan and Mark Sandler and Andrew Howard and Quoc V. Le},
    year={2018},
    eprint={1807.11626},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{s2018mobilenetv2,
    title={MobileNetV2: Inverted Residuals and Linear Bottlenecks},
    author={Mark Sandler and Andrew Howard and Menglong Zhu and Andrey Zhmoginov and Liang-Chieh Chen},
    year={2018},
    eprint={1801.04381},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{szegedy2016inceptionv4,
    title={Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning},
    author={Christian Szegedy and Sergey Ioffe and Vincent Vanhoucke and Alex Alemi},
    year={2016},
    eprint={1602.07261},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{simonyan2014deep,
    title={Very Deep Convolutional Networks for Large-Scale Image Recognition},
    author={Karen Simonyan and Andrew Zisserman},
    year={2014},
    eprint={1409.1556},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{tan2019efficientnet,
    title={EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
    author={Mingxing Tan and Quoc V. Le},
    year={2019},
    eprint={1905.11946},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{xie2019adversarial,
    title={Adversarial Examples Improve Image Recognition},
    author={Cihang Xie and Mingxing Tan and Boqing Gong and Jiang Wang and Alan Yuille and Quoc V. Le},
    year={2019},
    eprint={1911.09665},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
@misc{touvron2019fixing,
    title={Fixing the train-test resolution discrepancy},
    author={Hugo Touvron and Andrea Vedaldi and Matthijs Douze and Hervé Jégou},
    year={2019},
    eprint={1906.06423},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
@misc{alex2019large,
    title={Large Scale Learning of General Visual Representations for Transfer},
    author={Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Joan Puigcerver and Jessica Yung and Sylvain Gelly and Neil Houlsby},
    year={2019},
    eprint={1912.11370},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}


@article{Kim2018,
abstract = {Recently developed object detectors employ a convolutional neural network (CNN) by gradually increasing the number of feature layers with a pyramidal shape instead of using a featurized image pyramid. However, the different abstraction levels of CNN feature layers often limit the detection performance, especially on small objects. To overcome this limitation, we propose a CNN-based object detection architecture, referred to as a parallel feature pyramid (FP) network (PFPNet), where the FP is constructed by widening the network width instead of increasing the network depth. First, we adopt spatial pyramid pooling and some additional feature transformations to generate a pool of feature maps with different sizes. In PFPNet, the additional feature transformation is performed in parallel, which yields the feature maps with similar levels of semantic abstraction across the scales. We then resize the elements of the feature pool to a uniform size and aggregate their contextual information to generate each level of the final FP. The experimental results confirmed that PFPNet increases the performance of the latest version of the single-shot multi-box detector (SSD) by mAP of 6.4{\%} AP and especially, 7.8{\%} AP small on the MS-COCO dataset.},
author = {Kim, Seung Wook and Kook, Hyong Keun and Sun, Jee Young and Kang, Mun Cheon and Ko, Sung Jea},
doi = {10.1007/978-3-030-01228-1_15},
file = {:home/juliosilva/Desktop/Artigos/Tabelas/object/Seung-Wook{\_}Kim{\_}Parallel{\_}Feature{\_}Pyramid{\_}ECCV{\_}2018{\_}paper.pdf:pdf},
isbn = {9783030012274},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Feature pyramid,Real-time object detection},
mendeley-groups = {Capitulo 2 - tabelas},
pages = {239--256},
title = {{Parallel Feature Pyramid Network for Object Detection}},
volume = {11209 LNCS},
year = {2018}
}


@misc{shrivastava2016skip,
    title={Beyond Skip Connections: Top-Down Modulation for Object Detection},
    author={Abhinav Shrivastava and Rahul Sukthankar and Jitendra Malik and Abhinav Gupta},
    year={2016},
    eprint={1612.06851},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@article{Wang2019,
abstract = {Region anchors are the cornerstone of modern object detection techniques. State-of-the-art detectors mostly rely on a dense anchoring scheme, where anchors are sampled uniformly over the spatial domain with a predefined set of scales and aspect ratios. In this paper, we revisit this foundational stage. Our study shows that it can be done much more effectively and efficiently. Specifically, we present an alternative scheme, named Guided Anchoring, which leverages semantic features to guide the anchoring. The proposed method jointly predicts the locations where the center of objects of interest are likely to exist as well as the scales and aspect ratios at different locations. On top of predicted anchor shapes, we mitigate the feature inconsistency with a feature adaption module. We also study the use of high-quality proposals to improve detection performance. The anchoring scheme can be seamlessly integrated into proposal methods and detectors. With Guided Anchoring, we achieve 9.1{\%} higher recall on MS COCO with 90{\%} fewer anchors than the RPN baseline. We also adopt Guided Anchoring in Fast R-CNN, Faster R-CNN and RetinaNet, respectively improving the detection mAP by 2.2{\%}, 2.7{\%} and 1.2{\%}. Code is available at https://github.com/open-mmlab/mmdetection.},
archivePrefix = {arXiv},
arxivId = {1901.03278},
author = {Wang, Jiaqi and Chen, Kai and Yang, Shuo and Loy, Chen Change and Lin, Dahua},
doi = {10.1109/CVPR.2019.00308},
eprint = {1901.03278},
file = {:home/juliosilva/Desktop/Artigos/Tabelas/object/region{\_}proposal.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Categorization,Recognition: Detection,Retrieval},
mendeley-groups = {Capitulo 2 - tabelas},
pages = {2960--2969},
title = {{Region proposal by guided anchoring}},
volume = {2019-June},
year = {2019}
}

@article{Cai2018,
abstract = {In object detection, an intersection over union (IoU) threshold is required to define positives and negatives. An object detector, trained with low IoU threshold, e.g. 0.5, usually produces noisy detections. However, detection performance tends to degrade with increasing the IoU thresholds. Two main factors are responsible for this: 1) overfitting during training, due to exponentially vanishing positive samples, and 2) inference-time mismatch between the IoUs for which the detector is optimal and those of the input hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, is proposed to address these problems. It consists of a sequence of detectors trained with increasing IoU thresholds, to be sequentially more selective against close false positives. The detectors are trained stage by stage, leveraging the observation that the output of a detector is a good distribution for training the next higher quality detector. The resampling of progressively improved hypotheses guarantees that all detectors have a positive set of examples of equivalent size, reducing the overfitting problem. The same cascade procedure is applied at inference, enabling a closer match between the hypotheses and the detector quality of each stage. A simple implementation of the Cascade R-CNN is shown to surpass all single-model object detectors on the challenging COCO dataset. Experiments also show that the Cascade R-CNN is widely applicable across detector architectures, achieving consistent gains independently of the baseline detector strength. The code is available at https://github.com/zhaoweicai/cascade-rcnn.},
archivePrefix = {arXiv},
arxivId = {1712.00726},
author = {Cai, Zhaowei and Vasconcelos, Nuno},
doi = {10.1109/CVPR.2018.00644},
eprint = {1712.00726},
file = {:home/juliosilva/Desktop/Artigos/Tabelas/object/cascade{\_}rcnn.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Capitulo 2 - tabelas},
pages = {6154--6162},
title = {{Cascade R-CNN: Delving into High Quality Object Detection}},
year = {2018}
}

@article{Zhao2019,
abstract = {Feature pyramids are widely exploited by both the state-of-the-art one-stage object detectors (e.g., DSSD, RetinaNet, RefineDet) and the two-stage object detectors (e.g., Mask RCNN, DetNet) to alleviate the problem arising from scale variation across object instances. Although these object detectors with feature pyramids achieve encouraging results, they have some limitations due to that they only simply construct the feature pyramid according to the inherent multiscale, pyramidal architecture of the backbones which are originally designed for object classification task. Newly, in this work, we present Multi-Level Feature Pyramid Network (MLFPN) to construct more effective feature pyramids for detecting objects of different scales. First, we fuse multi-level features (i.e. multiple layers) extracted by backbone as the base feature. Second, we feed the base feature into a block of alternating joint Thinned U-shape Modules and Feature Fusion Modules and exploit the decoder layers of each Ushape module as the features for detecting objects. Finally, we gather up the decoder layers with equivalent scales (sizes) to construct a feature pyramid for object detection, in which every feature map consists of the layers (features) from multiple levels. To evaluate the effectiveness of the proposed MLFPN, we design and train a powerful end-to-end one-stage object detector we call M2Det by integrating it into the architecture of SSD, and achieve better detection performance than state-of-the-art one-stage detectors. Specifically, on MSCOCO benchmark, M2Det achieves AP of 41.0 at speed of 11.8 FPS with single-scale inference strategy and AP of 44.2 with multi-scale inference strategy, which are the new stateof-the-art results among one-stage detectors. The code will be made available on https://github.com/qijiezhao/M2Det.},
archivePrefix = {arXiv},
arxivId = {1811.04533},
author = {Zhao, Qijie and Sheng, Tao and Wang, Yongtao and Tang, Zhi and Chen, Ying and Cai, Ling and Ling, Haibin},
doi = {10.1609/aaai.v33i01.33019259},
eprint = {1811.04533},
file = {:home/juliosilva/Desktop/Artigos/Tabelas/object/M2Det.pdf:pdf},
issn = {2159-5399},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
mendeley-groups = {Capitulo 2 - tabelas},
pages = {9259--9266},
title = {{M2Det: A Single-Shot Object Detector Based on Multi-Level Feature Pyramid Network}},
volume = {33},
year = {2019}
}

@article{Mahajan2018,
abstract = {State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards “small”. Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4{\%} (97.6{\%} top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance.},
archivePrefix = {arXiv},
arxivId = {1805.00932},
author = {Mahajan, Dhruv and Girshick, Ross and Ramanathan, Vignesh and He, Kaiming and Paluri, Manohar and Li, Yixuan and Bharambe, Ashwin and van der Maaten, Laurens},
doi = {10.1007/978-3-030-01216-8_12},
eprint = {1805.00932},
file = {:home/juliosilva/Desktop/Artigos/Tabelas/object/limits{\_}supervised.pdf:pdf},
isbn = {9783030012151},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {Capitulo 2 - tabelas},
pages = {185--201},
title = {{Exploring the Limits of Weakly Supervised Pretraining}},
volume = {11206 LNCS},
year = {2018}
}



@online{Detectron2018,
  author =       {Ross Girshick and Ilija Radosavovic and Georgia Gkioxari and
                  Piotr Doll\'{a}r and Kaiming He},
  title =        {Detectron},
  url= {https://github.com/facebookresearch/detectron},
  urldate = {2020-03-09},
  year =         {2018}
}

@article{Li2019,
abstract = {Scale variation is one of the key challenges in object detection. In this work, we first present a controlled experiment to investigate the effect of receptive fields for scale variation in object detection. Based on the findings from the exploration experiments, we propose a novel Trident Network (TridentNet) aiming to generate scale-specific feature maps with a uniform representational power. We construct a parallel multi-branch architecture in which each branch shares the same transformation parameters but with different receptive fields. Then, we adopt a scale-aware training scheme to specialize each branch by sampling object instances of proper scales for training. As a bonus, a fast approximation version of TridentNet could achieve significant improvements without any additional parameters and computational cost compared with the vanilla detector. On the COCO dataset, our TridentNet with ResNet-101 backbone achieves state-of-the-art single-model results of 48.4 mAP. Codes are available at https://git.io/fj5vR.},
archivePrefix = {arXiv},
arxivId = {1901.01892},
author = {Li, Yanghao and Chen, Yuntao and Wang, Naiyan and Zhang, Zhaoxiang},
eprint = {1901.01892},
file = {:home/juliosilva/Desktop/Artigos/Tabelas/object/scale-aware.pdf:pdf},
mendeley-groups = {Capitulo 2 - tabelas},
title = {{Scale-Aware Trident Networks for Object Detection}},
url = {http://arxiv.org/abs/1901.01892},
year = {2019}
}


@article{Zhang2019,
abstract = {Object detection has been dominated by anchor-based detectors for several years. Recently, anchor-free detectors have become popular due to the proposal of FPN and Focal Loss. In this paper, we first point out that the essential difference between anchor-based and anchor-free detection is actually how to define positive and negative training samples, which leads to the performance gap between them. If they adopt the same definition of positive and negative samples during training, there is no obvious difference in the final performance, no matter regressing from a box or a point. This shows that how to select positive and negative training samples is important for current object detectors. Then, we propose an Adaptive Training Sample Selection (ATSS) to automatically select positive and negative samples according to statistical characteristics of object. It significantly improves the performance of anchor-based and anchor-free detectors and bridges the gap between them. Finally, we discuss the necessity of tiling multiple anchors per location on the image to detect objects. Extensive experiments conducted on MS COCO support our aforementioned analysis and conclusions. With the newly introduced ATSS, we improve state-of-the-art detectors by a large margin to {\$}50.7\backslash{\%}{\$} AP without introducing any overhead. The code is available at https://github.com/sfzhang15/ATSS},
archivePrefix = {arXiv},
arxivId = {1912.02424},
author = {Zhang, Shifeng and Chi, Cheng and Yao, Yongqiang and Lei, Zhen and Li, Stan Z.},
eprint = {1912.02424},
file = {:home/juliosilva/Desktop/Artigos/Tabelas/object/BridgingTheGap.pdf:pdf},
mendeley-groups = {Capitulo 2 - tabelas},
number = {2},
title = {{Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection}},
url = {http://arxiv.org/abs/1912.02424},
year = {2019}
}




@article{Tan2019,
abstract = {Model efficiency has become increasingly important in computer vision. In this paper, we systematically study various neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multi-scale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations, we have developed a new family of object detectors, called EfficientDet, which consistently achieve an order-of-magnitude better efficiency than prior art across a wide spectrum of resource constraints. In particular, without bells and whistles, our EfficientDet-D7 achieves stateof-the-art 51.0 mAP on COCO dataset with 52M parameters and 326B FLOPS1 , being 4x smaller and using 9.3x fewer FLOPS yet still more accurate (+0.3{\%} mAP) than the best previous detector.},
archivePrefix = {arXiv},
arxivId = {1911.09070},
author = {Tan, Mingxing and Pang, Ruoming and Le, Quoc V.},
eprint = {1911.09070},
file = {:home/juliosilva/Desktop/Artigos/Tabelas/efficientDet.pdf:pdf},
mendeley-groups = {Capitulo 2 - tabelas},
title = {{EfficientDet: Scalable and Efficient Object Detection}},
url = {http://arxiv.org/abs/1911.09070},
year = {2019}
}


@article{Xie2019,
abstract = {We present a simple self-training method that achieves 88.4{\%} top-1 accuracy on ImageNet, which is 2.0{\%} better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0{\%} to 83.7{\%}, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. To achieve this result, we first train an EfficientNet model on labeled ImageNet images and use it as a teacher to generate pseudo labels on 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the generation of the pseudo labels, the teacher is not noised so that the pseudo labels are as accurate as possible. However, during the learning of the student, we inject noise such as dropout, stochastic depth and data augmentation via RandAugment to the student so that the student generalizes better than the teacher.},
archivePrefix = {arXiv},
arxivId = {1911.04252},
author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V.},
eprint = {1911.04252},
file = {:home/juliosilva/Desktop/Artigos/Tabelas/noisyStudent.pdf:pdf},
mendeley-groups = {Capitulo 2 - tabelas},
title = {{Self-training with Noisy Student improves ImageNet classification}},
url = {http://arxiv.org/abs/1911.04252},
year = {2019}
}

@article{Liu2019,
abstract = {In existing CNN based detectors, the backbone network is a very important component for basic feature extraction, and the performance of the detectors highly depends on it. In this paper, we aim to achieve better detection performance by building a more powerful backbone from existing backbones like ResNet and ResNeXt. Specifically, we propose a novel strategy for assembling multiple identical backbones by composite connections between the adjacent backbones, to form a more powerful backbone named Composite Backbone Network (CBNet). In this way, CBNet iteratively feeds the output features of the previous backbone, namely high-level features, as part of input features to the succeeding backbone, in a stage-by-stage fashion, and finally the feature maps of the last backbone (named Lead Backbone) are used for object detection. We show that CBNet can be very easily integrated into most state-of-the-art detectors and significantly improve their performances. For example, it boosts the mAP of FPN, Mask R-CNN and Cascade R-CNN on the COCO dataset by about 1.5 to 3.0 percent. Meanwhile, experimental results show that the instance segmentation results can also be improved. Specially, by simply integrating the proposed CBNet into the baseline detector Cascade Mask R-CNN, we achieve a new state-of-the-art result on COCO dataset (mAP of 53.3) with single model, which demonstrates great effectiveness of the proposed CBNet architecture. Code will be made available on https://github.com/PKUbahuangliuhe/CBNet.},
archivePrefix = {arXiv},
arxivId = {1909.03625},
author = {Liu, Yudong and Wang, Yongtao and Wang, Siwei and Liang, TingTing and Zhao, Qijie and Tang, Zhi and Ling, Haibin},
eprint = {1909.03625},
file = {:home/juliosilva/Desktop/Artigos/Tabelas/cbnet.pdf:pdf},
mendeley-groups = {Capitulo 2 - tabelas},
title = {{CBNet: A Novel Composite Backbone Network Architecture for Object Detection}},
url = {http://arxiv.org/abs/1909.03625},
year = {2019}
}


@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
file = {:home/juliosilva/Desktop/Artigos/Tabelas/imageNet{\_}challenge.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
mendeley-groups = {Capitulo 2 - tabelas},
number = {3},
pages = {211--252},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}

@article{Xie2017,
abstract = {We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online1.},
archivePrefix = {arXiv},
arxivId = {1611.05431},
author = {Xie, Saining and Girshick, Ross and Doll{\'{a}}r, Piotr and Tu, Zhuowen and He, Kaiming},
doi = {10.1109/CVPR.2017.634},
eprint = {1611.05431},
file = {:home/juliosilva/Desktop/Artigos/Tabelas/ResNetXt.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
mendeley-groups = {Capitulo 2 - tabelas},
pages = {5987--5995},
title = {{Aggregated residual transformations for deep neural networks}},
volume = {2017-January},
year = {2017}
}



%%%%%%%%%%%%%%%%%%%%%%  Chapter 2 - article
@misc{girshick2015fast,
    title={Fast R-CNN},
    author={Ross Girshick},
    year={2015},
    eprint={1504.08083},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
@misc{ren2015faster,
    title={Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
    author={Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},
    year={2015},
    eprint={1506.01497},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
@misc{he2017mask,
    title={Mask R-CNN},
    author={Kaiming He and Georgia Gkioxari and Piotr Dollár and Ross Girshick},
    year={2017},
    eprint={1703.06870},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
@misc{lin2016feature,
    title={Feature Pyramid Networks for Object Detection},
    author={Tsung-Yi Lin and Piotr Dollár and Ross Girshick and Kaiming He and Bharath Hariharan and Serge Belongie},
    year={2016},
    eprint={1612.03144},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
@article{Liu2016,
abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For {\$}300\backslashtimes 300{\$} input, SSD achieves 72.1{\%} mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for {\$}500\backslashtimes 500{\$} input, SSD achieves 75.1{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
archivePrefix = {arXiv},
arxivId = {arXiv:1512.02325v5},
author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng Yang and Berg, Alexander C.},
doi = {10.1007/978-3-319-46448-0_2},
eprint = {arXiv:1512.02325v5},
file = {:home/juliosilva/Desktop/Artigos/ssd.pdf:pdf},
isbn = {9783319464473},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Convolutional neural network,Real-time object detection},
mendeley-groups = {Capitulo 2 - redes},
pages = {21--37},
title = {{SSD Net}},
volume = {9905 LNCS},
year = {2016}
}


@article{OShea2015,
abstract = {The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs. This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.},
archivePrefix = {arXiv},
arxivId = {1511.08458},
author = {O'Shea, Keiron and Nash, Ryan},
eprint = {1511.08458},
file = {:home/juliosilva/Desktop/Artigos/introductioncnn.pdf:pdf},
mendeley-groups = {Capitulo 2 - redes},
number = {November},
title = {{An Introduction to Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1511.08458},
year = {2015}
}


@article{Ribeiro,
author = {Ribeiro, Ricardo Ferreira and Deti, Ieeta},
file = {:home/juliosilva/Desktop/Escrita{\_}Tese/Ricardo/Data{\_}Mining{\_}Project{\_}Report.pdf:pdf},
mendeley-groups = {Capitulo 2 - artigos},
title = {{Object Recognition with Convolutional Neural Networks}}
}



@article{weng2017detection1,
  title   = "Object Detection for Dummies Part 1: Gradient Vector, HOG, and SS",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io/lil-log",
  year    = "2017",
  url     = "http://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html"
}

@article{weng2017detection3,
  title   = "Object Detection for Dummies Part 3: R-CNN Family",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io/lil-log",
  year    = "2017",
  url     = "http://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html"
}


@article{Everingham2010,
abstract = {The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension. {\textcopyright} 2009 Springer Science+Business Media, LLC.},
author = {Everingham, Mark and {Van Gool}, Luc and Williams, Christopher K.I. and Winn, John and Zisserman, Andrew},
doi = {10.1007/s11263-009-0275-4},
file = {:home/juliosilva/Desktop/Artigos/Datasets/Pascal-VOC.pdf:pdf},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Database,Object detection,Object recognition},
mendeley-groups = {Capitulo 2 - artigos},
number = {2},
pages = {303--338},
title = {{The pascal visual object classes (VOC) challenge}},
volume = {88},
year = {2010}
}


@article{Agarwal2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1809.03193v2},
author = {Agarwal, Shivang and Ogier, Jean},
eprint = {arXiv:1809.03193v2},
file = {:home/juliosilva/Desktop/Artigos/Recent Advances in Object Detection in the Age of Deep
Convolutional Neural Networks
.pdf:pdf},
mendeley-groups = {Capitulo 2 - artigos},
title = {{Recent Advances in Object Detection in the Age of Deep Convolutional Neural Networks}},
year = {2019}
}

@article{Tiwari2013,
abstract = {Feature Extraction is one of the most popular research areas in the field of image analysis as it is a prime requirement in order to represent an object. An object is represented by a group of features in form of a feature vector. This feature vector is used to recognize objects and classify them. Previous works have proposed various feature extraction techniques to find the feature vector. This paper provides a comprehensive framework of various feature extraction techniques and their use in object recognition and classification. It also provides their comparison. Various techniques have been considered and their pros and cons along with the method of implementation and detailed experimental results have been discussed.},
author = {Tiwari, Aastha and Goswami, Anil Kumar and Saraswat, Mansi},
file = {:home/juliosilva/Desktop/feature-extraction-for-object-recognition-and-image-classification-IJERTV2IS100491.pdf:pdf},
isbn = {2278-0181},
journal = {International Journal of Engineering Research {\&} Technology (IJERT)},
keywords = {classification,extraction,feature,image,object,recognition},
mendeley-groups = {Capitulo 2 - artigos},
number = {10},
pages = {1238--1246},
title = {{Feature Extraction for Object Recognition and Image Classification}},
url = {http://www.ijert.org/view-pdf/5674/feature-extraction-for-object-recognition-and-image-classification},
volume = {2},
year = {2013}
}


@article{Feng2019,
abstract = {The field of computer vision is experiencing a great-leap-forward development today. This paper aims at providing a comprehensive survey of the recent progress on computer vision algorithms and their corresponding hardware implementations. In particular, the prominent achievements in computer vision tasks such as image classification, object detection and image segmentation brought by deep learning techniques are highlighted. On the other hand, review of techniques for implementing and optimizing deep-learning-based computer vision algorithms on GPU, FPGA and other new generations of hardware accelerators are presented to facilitate real-time and/or energy-efficient operations. Finally, several promising directions for future research are presented to motivate further development in the field.},
author = {Feng, Xin and Jiang, Youni and Yang, Xuejiao and Du, Ming and Li, Xin},
doi = {10.1016/j.vlsi.2019.07.005},
file = {:home/juliosilva/Desktop/Tese{\_}windows/Artigos/Computer vision algorithms and hardware implementations$\backslash$: A survey.pdf:pdf},
isbn = {1064699383},
issn = {01679260},
journal = {Integration},
keywords = {Artificial intelligence,Computer vision,Deep convolutional neural network,Hardware accelerator},
mendeley-groups = {Capitulo 2 - artigos},
number = {August},
pages = {309--320},
publisher = {Elsevier Ltd},
title = {{Computer vision algorithms and hardware implementations: A survey}},
url = {https://doi.org/10.1016/j.vlsi.2019.07.005},
volume = {69},
year = {2019}
}

@article{Huang2012,
abstract = {The World-Wide Web provides tremendous resources in Multimedia Data, Ubiquitous Interconnection, and Storage/Computing Power. In this short note, we raise but not answer the question: Can the WWW help bridge the Semantic Gap in Computer Vision? Even if the answer turns out to be NO, we hope that by exploring these resources, we may gain a deeper understanding of the Semantic Gap challenge. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Huang, Thomas S.},
doi = {10.1016/j.imavis.2011.10.001},
file = {:home/juliosilva/Desktop/1-s2.0-S026288561100103X-main.pdf:pdf},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {New frontiers in computer vision,Opinion paper},
mendeley-groups = {Capitulo 2 - artigos},
number = {8},
pages = {463--464},
publisher = {Elsevier B.V.},
title = {{Can the world-wide web bridge the semantic gap?}},
url = {http://dx.doi.org/10.1016/j.imavis.2011.10.001},
volume = {30},
year = {2012}
}

@article{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model. {\textcopyright} 2014 Springer International Publishing.},
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
doi = {10.1007/978-3-319-10602-1_48},
eprint = {1405.0312},
file = {:home/juliosilva/Desktop/Tese{\_}windows/Artigos/Artigos{\_}Redes/Microsoft COCO$\backslash$: Common Objects in Context.pdf:pdf},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {Capitulo 2 - artigos},
number = {PART 5},
pages = {740--755},
title = {{Microsoft COCO: Common objects in context}},
volume = {8693 LNCS},
year = {2014}
}

@article{Language2015,
abstract = {Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked "What vehicle is the person riding?", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) to answer correctly that "the person is riding a horse-drawn carriage." In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically , our dataset contains over 108K images where each image has an average of 35 objects, 26 attributes, and 21 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answer pairs.},
author = {Language, Connecting and Using, Vision and Dense, Crowdsourced and Annotations, Image},
file = {:home/juliosilva/Desktop/Tese{\_}windows/Artigos/Artigos{\_}Redes/Visual{\_}Genome.pdf:pdf},
keywords = {Answering {\textperiodcentered},Attributes {\textperiodcentered},Computer,Crowdsourcing,Dataset {\textperiodcentered},Graph {\textperiodcentered},Image {\textperiodcentered},Knowledge {\textperiodcentered},Language {\textperiodcentered},Objects {\textperiodcentered},Question,Relationships {\textperiodcentered},Scene,Vision {\textperiodcentered}},
mendeley-groups = {Capitulo 2 - artigos},
title = {{Visual Genome - connected language and Vision using crowdsourced dense image annotations}},
url = {https://visualgenome.org},
year = {2015}
}

@article{Takamitsu1978,
author = {Takamitsu, Y. and Orita, Y.},
doi = {10.14842/jpnjnephrol1959.20.1221},
file = {:home/juliosilva/Desktop/imagenet{\_}cvpr09.pdf:pdf},
issn = {03852385},
journal = {Japanese Journal of Nephrology},
keywords = {a large-scale hierarchical image,cs,database,dept,edu,fei-fei,feifeili,genet,jia deng,jiadeng,jial,kai li and li,li,li-jia li,of computer science,princeton,princeton university,richard socher,rsocher,usa,wdong,wei dong},
mendeley-groups = {Capitulo 2 - artigos},
number = {11},
pages = {1221--1227},
title = {{Effect of glomerular change on the electrolyte reabsorption of the renal tubule in glomerulonephritis (author's transl)}},
volume = {20},
year = {1978}
}

@article{Kuznetsova2018,
abstract = {We present Open Images V4, a dataset of 9.2M images with unified annotations for image classification, object detection and visual relationship detection. The images have a Creative Commons Attribution license that allows to share and adapt the material, and they have been collected from Flickr without a predefined list of class names or tags, leading to natural class statistics and avoiding an initial design bias. Open Images V4 offers large scale across several dimensions: 30.1M image-level labels for 19.8k concepts, 15.4M bounding boxes for 600 object classes, and 375k visual relationship annotations involving 57 classes. For object detection in particular, we provide 15x more bounding boxes than the next largest datasets (15.4M boxes on 1.9M images). The images often show complex scenes with several objects (8 annotated objects per image on average). We annotated visual relationships between them, which support visual relationship detection, an emerging task that requires structured reasoning. We provide in-depth comprehensive statistics about the dataset, we validate the quality of the annotations, and we study how the performance of many modern models evolves with increasing amounts of training data. We hope that the scale, quality, and variety of Open Images V4 will foster further research and innovation even beyond the areas of image classification, object detection, and visual relationship detection.},
archivePrefix = {arXiv},
arxivId = {1811.00982},
author = {Kuznetsova, Alina and Rom, Hassan and Alldrin, Neil and Uijlings, Jasper and Krasin, Ivan and Pont-Tuset, Jordi and Kamali, Shahab and Popov, Stefan and Malloci, Matteo and Duerig, Tom and Ferrari, Vittorio},
eprint = {1811.00982},
file = {:home/juliosilva/Desktop/openimage.pdf:pdf},
keywords = {alina kuznetsova hassan rom,and visual relationship detection,at scale,ivan krasin jordi pont-tuset,neil alldrin jasper uijlings,object detection,the open images dataset,unified image classification,v submission in review,v4},
mendeley-groups = {Capitulo 2 - artigos},
pages = {1--20},
title = {{The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale}},
url = {http://arxiv.org/abs/1811.00982},
year = {2018}
}

@article{Culjak2012,
abstract = {The purpose of this paper is to introduce and quickly make a reader familiar with OpenCV (Open Source Computer Vision) basics without having to go through the lengthy reference manuals and books. OpenCV is an open source library for image and video analysis, originally introduced more than decade ago by Intel. Since then, a number of programmers have contributed to the most recent library developments. The latest major change took place in 2009 (OpenCV 2) which includes main changes to the C++ interface. Nowadays the library has {\textgreater}2500 optimized algorithms. It is extensively used around the world, having {\textgreater}2.5M downloads and {\textgreater}40K people in the user group. Regardless of whether one is a novice C++ programmer or a professional software developer, unaware of OpenCV, the main library content should be interesting for the graduate students and researchers in image processing and computer vision areas. To master every library element it is necessary to consult many books available on the topic of OpenCV. However, reading such more comprehensive material should be easier after comprehending some basics about OpenCV from this paper. {\textcopyright} 2012 MIPRO.},
author = {Culjak, Ivan and Abram, David and Pribanic, Tomislav and Dzapo, Hrvoje and Cifrek, Mario},
doi = {10.1201/b12208-9},
file = {:home/juliosilva/Desktop/Artigos/Libraries/OpenCV.pdf:pdf},
isbn = {9789532330724},
journal = {MIPRO 2012 - 35th International Convention on Information and Communication Technology, Electronics and Microelectronics - Proceedings},
keywords = {2012,a brief introduction to,and computing,croatia,david abram,faculty of electrical engineering,hrvoje dzapo,ivan culjak,mario cifrek,may 21-25,opatija,opencv,ro 2012,tomislav pribanic,university of zagreb,zagreb},
mendeley-groups = {Capitulo 2 - artigos},
pages = {1725--1730},
publisher = {IEEE},
title = {{A brief introduction to OpenCV}},
year = {2012}
}


@article{Guo2019,
abstract = {We present GluonCV and GluonNLP, the deep learning toolkits for computer vision and natural language processing based on Apache MXNet (incubating). These toolkits provide state-of-the-art pre-trained models, training scripts, and training logs, to facilitate rapid prototyping and promote reproducible research. We also provide modular APIs with flexible building blocks to enable efficient customization. Leveraging the MXNet ecosystem, the deep learning models in GluonCV and GluonNLP can be deployed onto a variety of platforms with different programming languages. Benefiting from open source under the Apache 2.0 license, GluonCV and GluonNLP have attracted 100 contributors worldwide on GitHub. Models of GluonCV and GluonNLP have been downloaded for more than 1.6 million times in fewer than 10 months.},
archivePrefix = {arXiv},
arxivId = {1907.04433},
author = {Guo, Jian and He, He and He, Tong and Lausen, Leonard and Li, Mu and Lin, Haibin and Shi, Xingjian and Wang, Chenguang and Xie, Junyuan and Zha, Sheng and Zhang, Aston and Zhang, Hang and Zhang, Zhi and Zhang, Zhongyue and Zheng, Shuai},
eprint = {1907.04433},
file = {:home/juliosilva/Desktop/Artigos/Libraries/GluonCV.pdf:pdf},
keywords = {computer vision and natural,deep learning in,language processing,oncv and gluonnlp},
mendeley-groups = {Capitulo 2 - artigos},
pages = {1--6},
title = {{GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural Language Processing}},
url = {http://arxiv.org/abs/1907.04433},
year = {2019}
}


@techreport{Abadi,
abstract = {TensorFlow [1] is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition , computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the Ten-sorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'{e}}, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'{e}}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang and Research, Google},
file = {:home/juliosilva/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abadi et al. - Unknown - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
mendeley-groups = {Capitulo 2 - artigos},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {www.tensorflow.org.}
}

@article{Dignam1983,
author = {Dignam, John D. and Martin, Paul L. and Shastry, Barkur S. and Roeder, Robert G.},
doi = {10.1016/0076-6879(83)01039-3},
file = {:home/juliosilva/Desktop/Artigos/Libraries/tensorflow2.pdf:pdf},
isbn = {9781931971331},
issn = {15577988},
journal = {Methods in Enzymology},
mendeley-groups = {Capitulo 2 - artigos},
number = {C},
pages = {582--598},
pmid = {6888276},
title = {{Eukaryotic gene transcription with purified components}},
volume = {101},
year = {1983}
}

@article{Iandola2016,
abstract = {Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet). The SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet},
archivePrefix = {arXiv},
arxivId = {1602.07360},
author = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
eprint = {1602.07360},
file = {:home/juliosilva/Desktop/Artigos/Redes/SqueezeNet.pdf:pdf},
mendeley-groups = {Capitulo 2 - redes},
pages = {1--13},
title = {{SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and {\textless}0.5MB model size}},
url = {http://arxiv.org/abs/1602.07360},
year = {2016}
}

@online{squeezenetweb,
author = {Sik-Ho Tsang},
mendeley-groups = {Capitulo 2 - web},
title = {{Review: SqueezeNet (Image Classification) - Towards Data Science}},
url = {https://towardsdatascience.com/review-squeezenet-image-classification-e7414825581a},
urldate = {2020-01-23}
}

@article{He2016,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
file = {:home/juliosilva/Desktop/Artigos/Redes/Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Capitulo 2 - redes},
pages = {770--778},
title = {{Deep residual learning for image recognition}},
volume = {2016-December},
year = {2016}
}


@article{Szegedy2016,
abstract = {Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2{\%} top-1 and 5:6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5{\%} top-5 error and 17:3{\%} top-1 error on the validation set and 3:6{\%} top-5 error on the official test set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
doi = {10.1109/CVPR.2016.308},
eprint = {1512.00567},
file = {:home/juliosilva/Desktop/Artigos/Redes/inceptionV3.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Capitulo 2 - redes},
pages = {2818--2826},
title = {{Rethinking the Inception Architecture for Computer Vision}},
volume = {2016-December},
year = {2016}
}

@article{Szegedy2016,
abstract = {Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2{\%} top-1 and 5:6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5{\%} top-5 error and 17:3{\%} top-1 error on the validation set and 3:6{\%} top-5 error on the official test set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
doi = {10.1109/CVPR.2016.308},
eprint = {1512.00567},
file = {:home/juliosilva/Desktop/Artigos/Redes/inceptionV3.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Capitulo 2 - redes},
pages = {2818--2826},
title = {{Rethinking the Inception Architecture for Computer Vision}},
volume = {2016-Decem},
year = {2016}
}

@article{Lin2017,
abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors.},
archivePrefix = {arXiv},
arxivId = {1708.02002},
author = {Lin, Tsung Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollar, Piotr},
doi = {10.1109/ICCV.2017.324},
eprint = {1708.02002},
file = {:home/juliosilva/Desktop/Artigos/Redes/retinanet.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
mendeley-groups = {Capitulo 2 - redes},
pages = {2999--3007},
title = {{Focal Loss for Dense Object Detection}},
volume = {2017-October},
year = {2017}
}

@article{Redmon2018,
abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
archivePrefix = {arXiv},
arxivId = {1804.02767},
author = {Redmon, Joseph and Farhadi, Ali},
eprint = {1804.02767},
file = {:home/juliosilva/Desktop/Artigos/Redes/YOLOv3.pdf:pdf},
mendeley-groups = {Capitulo 2 - redes},
title = {{YOLOv3: An Incremental Improvement}},
url = {http://arxiv.org/abs/1804.02767},
year = {2018}
}

@article{Yi2019,
abstract = {The existing real-time pedestrian detection method often loses part of the detection accuracy. For this reason, we proposed a real-time pedestrian detection algorithm based on tiny-yolov3. The proposed method uses K-means clustering on our training set to find the best priors. We improved the network structure of tiny-yolov3 to make it more accurate in pedestrian detection. From the experimental results, the proposed method has higher detection accuracy under the premise of satisfying real-time performance.},
author = {Yi, Zhang and Yongliang, Shen and Jun, Zhang},
doi = {10.1016/j.ijleo.2019.02.038},
file = {:home/juliosilva/Desktop/Artigos/Redes/An improved tiny-yolov3 pedestrian detection algorithm.pdf:pdf},
issn = {00304026},
journal = {Optik},
keywords = {Pedestrian detection,Real-time,k-means,tiny-yolov3},
mendeley-groups = {Capitulo 2 - redes},
number = {January},
pages = {17--23},
title = {{An improved tiny-yolov3 pedestrian detection algorithm}},
volume = {183},
year = {2019}
}



%%%%%%%%%%%%%%%%%%%%%%%%% NATURAL LANGUAGE PROCESSING %%%%%%%%%%%%%%%%%
@inproceedings{Pennington2014,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75{\%} on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.},
booktitle = {EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
doi = {10.3115/v1/d14-1162},
isbn = {9781937284961},
mendeley-groups = {NLP - web},
pages = {1532--1543},
publisher = {Association for Computational Linguistics (ACL)},
title = {{GloVe: Global vectors for word representation}},
year = {2014}
}


@article{Khurana2018,
abstract = {Natural language processing (NLP) has recently gained much attention for representing and analysing human language computationally. It has spread its applications in various fields such as machine translation, email spam detection, information extraction, summarization, medical, and question answering etc. The paper distinguishes four phases by discussing different levels of NLP and components of Natural Language Generation (NLG) followed by presenting the history and evolution of NLP, state of the art presenting the various applications of NLP and current trends and challenges.},
author = {Khurana, Diksha and Koli, Aditya and Khatter, Kiran and Singh, Sukhdev},
file = {:home/juliosilva/Desktop/Natural{\_}Language{\_}Processing{\_}State{\_}of{\_}The{\_}Art{\_}Curre.pdf:pdf},
journal = {arXiv preprint arXiv},
mendeley-groups = {NLP - artigos},
number = {August 2017},
title = {{Natural Language Processing : State of The Art , Current Trends and Challenges Natural Language Processing : State of The Art , Current Trends and Challenges Department of Computer Science and Engineering Manav Rachna International University , Faridabad-}},
year = {2018}
}

@article{Otter2018,
abstract = {Over the last several years, the field of natural language processing has been propelled forward by an explosion in the use of deep learning models. This survey provides a brief introduction to the field and a quick overview of deep learning architectures and methods. It then sifts through the plethora of recent studies and summarizes a large assortment of relevant contributions. Analyzed research areas include several core linguistic processing issues in addition to a number of applications of computational linguistics. A discussion of the current state of the art is then provided along with recommendations for future research in the field.},
archivePrefix = {arXiv},
arxivId = {1807.10854},
author = {Otter, Daniel W. and Medina, Julian R. and Kalita, Jugal K.},
eprint = {1807.10854},
file = {:home/juliosilva/Desktop/Deep Learning forNatural Language Processing.pdf:pdf},
mendeley-groups = {NLP - artigos},
number = {X},
pages = {1--22},
title = {{A Survey of the Usages of Deep Learning in Natural Language Processing}},
url = {http://arxiv.org/abs/1807.10854},
volume = {XX},
year = {2018}
}

@article{bojanowski2016enriching,
  title={Enriching Word Vectors with Subword Information},
  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1607.04606},
  year={2016}
}


%%%%%%%%%%%%%%%%%%%%%%%%% NATURAL LANGUAGE PROCESSING - web %%%%%%%%%%%%%%%%%

@online{MuratMustafa,
author = {{Murat Mustafa}},
mendeley-groups = {NLP - web},
title = {{GloVE | Mustafa Murat ARAT}},
url = {https://mmuratarat.github.io/2020-03-20/glove},
urldate = {2020-05-13}
}


@online{Brownlee2017,
abstract = {What Are Word Embeddings for Text? Jason Brownlee Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation. They are a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems. In this post, you will discover the word embedding approach for representing text data. After completing this post, you will know: What the word embedding approach for representing text is and how it differs from other feature extraction methods. That there are 3 main algorithms for learning a word embedding from text data. That you can either train a new embedding or use a pre-trained embedding on your natural language processing task.},
author = {Brownlee, Jason},
mendeley-groups = {NLP - web},
title = {{What are word embeddings for text?}},
url = {https://machinelearningmastery.com/what-are-word-embeddings/},
urldate = {2020-02-06},
year = {2017}
}

@online{Vidhya2017,
author = {Vidhya, Analytics},
booktitle = {Analytics Vidhya},
mendeley-groups = {NLP - web},
title = {{Understanding Word Embeddings: From Word2Vec to Count Vectors}},
url = {https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/},
urldate = {2020-02-06},
year = {2017}
}

@online{Wiki,
author = {Wiki, A.I.},
mendeley-groups = {NLP - web},
title = {{A Beginner's Guide to Word2Vec and Neural Word Embeddings | Skymind}},
url = {https://pathmind.com/wiki/word2vec https://skymind.ai/wiki/word2vec},
urldate = {2020-02-08}
}


@online{Batista2018,
author = {Batista, David},
mendeley-groups = {NLP - web},
title = {{[oorsig] Language Models and Contextualised Word Embeddings}},
url = {http://www.davidsbatista.net/blog/2018/12/06/Word{\_}Embeddings/ http://www.davidsbatista.net//blog/2018/12/06/Word{\_}Embeddings/},
urldate = {2020-02-10},
year = {2018}
}



@online{Spacy2017,
author = {Spacy},
mendeley-groups = {NLP - web},
title = {{spaCy 101: Everything you need to know {\textperiodcentered} spaCy Usage Documentation}},
url = {https://spacy.io/usage/spacy-101},
urldate = {2020-02-10},
year = {2017}
}

@online{word2vec_explained,
mendeley-groups = {NLP - web},
title = {{Word2Vec Explained Easily - InsightsBot}},
url = {http://www.insightsbot.com/word2vec-explained-easily/},
urldate = {2020-02-08}
}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%NATURAL LANGUAGE PROCESSING - article%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{Nguyen2016,
abstract = {In recent years, a deep learning model called convolutional neural network with an ability of ex-tracting features of high-level abstraction from minimum preprocessing data has been widely used. In this research, we proposed a new approach in classifying DNA sequences using the con-volutional neural network while considering these sequences as text data. We used one-hot vec-tors to represent sequences as input to the model; therefore, it conserves the essential position information of each nucleotide in sequences. Using 12 DNA sequence datasets, we evaluated our proposed model and achieved significant improvements in all of these datasets. This result has shown a potential of using convolutional neural network for DNA sequence to solve other se-quence problems in bioinformatics.},
author = {Nguyen, Ngoc Giang and Tran, Vu Anh and Ngo, Duc Luu and Phan, Dau and Lumbanraja, Favorisen Rosyking and Faisal, Mohammad Reza and Abapihi, Bahriddin and Kubo, Mamoru and Satou, Kenji},
doi = {10.4236/jbise.2016.95021},
file = {:home/juliosilva/Desktop/JBiSE{\_}2016042713533805.pdf:pdf},
issn = {1937-6871},
journal = {Journal of Biomedical Science and Engineering},
keywords = {DNA Sequence Classification, Deep Learning, Convol},
mendeley-groups = {NLP - artigos},
number = {05},
pages = {280--286},
title = {{DNA Sequence Classification by Convolutional Neural Network}},
volume = {09},
year = {2016}
}

@article{Mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
eprint = {1301.3781},
file = {:home/juliosilva/Desktop/Artigos/NLP/word2vec.pdf:pdf},
journal = {1st International Conference on Learning Representations, ICLR 2013 - Workshop Track Proceedings},
mendeley-groups = {NLP - artigos},
pages = {1--12},
title = {{Efficient estimation of word representations in vector space}},
year = {2013}
}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% CHAPTER INITIAL WORK %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@online {ImageAI,
    author = "Moses and John Olafenwa",
    title  = "ImageAI, an open source python library built to empower developers to build applications and systems  with self-contained Computer Vision capabilities",
    url    = "https://github.com/OlafenwaMoses/ImageAI",
    month  = "mar",
    year   = "2018--"
}


