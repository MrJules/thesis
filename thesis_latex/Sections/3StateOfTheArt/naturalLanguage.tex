\cleardoublepage


\chapter{Information Extraction From Text}
\label{ch:nlp}

As previously discussed, the problem of big data has become more relevant in the recent years.  This problem does not apply only to the ever growing amount of multimedia data created by smartphones but also to the growing presence of information in the form of news, corporate files, medical records, government documents, court hearing and social media. There is an ever increasing flood of information in an unstructured form. Natural Language Processing (NLP) is related to the usage of computation methods to process such data as a mode of communication used by humans.

``There are lot many processes involved in the pipeline of NLP. At the syntactic level, statements are segmented into words, punctuation (i.e.  tokens) and each token is assigned with its label in the form of noun, verb, adjective, adverb and so on (Part of Speech Tagging).  At the semantic level, each word is analyzed to get the meaningful representation of the sentence.  Hence, the basic task of NLP is to process the unstructured text and to produce a representation of its meaning.''  \cite{singh2018natural}.



Information Extraction (IE) from text is the process of extracting useful information from textual sources by implementing techniques of NLP. It can be defined as the act of efficiently and effectively analyze text and extract valuable and relevant knowledge from it in the form of structured information. ``The goal of IE is to extract salient facts about pre-specified types of events, entities, or relationships, in order to build more meaningful, rich representations of their semantic content, which can be used to populate databases that provide more structured input.'' \cite{singh2018natural}.

In this thesis, NLP is implemented to serve the purpose of creating an algorithm capable of extracting relevant information from a textual source. Furthermore, the algorithm must also be able to categorize the extracted data according pre-specified categories such as \enquote{locations} and \enquote{activities}. Additionally, NLP is also used for the computation of similarity values between extracted textual data and extracted visual labels from images.

This chapter starts with an introduction to Natural Language Processing in Section \ref{sec:nlp}. Section \ref{sec:num} explains the process of representing text in a numerical vector form while describing the concept of word embeddings. Static and contextualized word embedding models are introduced in Section \ref{sec:static} and \ref{sec:context} respectively. An overview on some of the available NLP libraries is presented in Section \ref{sec:libraries}. Finally, Section \ref{sec:final_remarks} gives the final remarks of this chapter.

\newpage
\section{Natural Language Processing}
\label{sec:nlp}

Natural language processing is a subfield of linguistics, computer science, information engineering and artificial intelligence, which is devoted to the engineering of computational models and processes to give the ability of human-like comprehension of texts/languages to computers \cite{Khurana2018}. 

Human language is extremely complex and rarely precise. In order to understand it, not only it is required to comprehend what words alone mean, but also how linking them together creates meaning. The nature of the human language makes NLP one of the most difficult tasks in computer science. 

Figure \ref{fig:nlp_class} shows the classification of NLP, which consists in two major components, Natural Language Understanding (NLU) and Natural Language Generation (NLG) \cite{Khurana2018}.  \par




\begin{figure}[htb]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.7\textwidth]{Sections/3StateOfTheArt/3_images/NLP_diagram.png}
    \caption[Classification of NLP.]{Classification of NLP \cite{Khurana2018}.}  
    \label{fig:nlp_class} 
\end{figure}

Natural Language Understanding is the process of understanding text. It is related to the science of Linguistic that studies the meaning of languages, context and various forms of language. 

Natural Language Generation is the process of generating text, sentences and
paragraphs that are meaningful from an internal representation \cite{Khurana2018}. 

Using the visual representation of Figure \ref{fig:nlp_class} the important terminologies of NLP are as follows \cite{Khurana2018}:

\begin{itemize}
    \item \textbf{Phonology}: The part of Linguistics which refers to the systematic arrangement of sound;
    \item \textbf{Morphology}: In linguistics, morphology is the study of words, how they are formed, and their relationship to other words in the same language. The different parts of the word represent the smallest units of meaning known as Morphemes;
    \item \textbf{Lexical}: In Lexical the focus is the interpretation of the meaning of individual words;
    \item \textbf{Syntax}: Syntax refers to the study of the grammatical structure of the sentence;
    \item  \textbf{Semantic}: Semantic processing determines the possible meanings of a
    sentence by pivoting on the interactions among word-level meanings in the sentence;
    \item \textbf{Discourse}: Discourse focuses on the properties of the text as a whole that convey meaning by making connections between component sentences;
    \item \textbf{Pragmatic}: Subfield of linguistics that studies the ways in which the context of a sentence contributes to the meaning. 
    
\end{itemize}

The field of NLP can be divided in two broad sub-areas: core areas and application areas. The core areas address fundamental problems such as language modeling, morphological processing, parsing and semantic processing \cite{Otter2018}. 



\begin{itemize}
    \item Language modeling is considered the most important task in NLP and an essential piece in any application of NLP. It is the process of creating a model capable of predicting words or simple linguistic components given previous words or components. It can capture syntactic and semantic relationships among words or components in a linear neighborhood, making it useful for tasks such as machine translation and text summarization.
    \item Morphological processing is the process of finding segments within single words,  including roots and stems, prefixes and suffixes.
    \item Parsing examines how different words and phrases relate to each other.
    \item Semantic processing is the task of understanding the meaning of words and phrases. This is done recurring to  word embedding models, like Word2Vec. This will be further discussed in this chapter.
\end{itemize}





The application areas address topics such as extraction of useful information from text (e.g named entities and relations), translation of text, summarization of written documents, automatic answering of questions, chat bots, email spam detection and many others \cite{Otter2018}.




\section{Numerical Representation of Text}
\label{sec:num}

\par Machine learning algorithms and most of all deep learning architectures are incapable of processing strings of text, this is because they require numbers (vectors) as an input in order to perform linear algebra operations \cite{Vidhya2017} which is not possible with words. A human can easily tell that the word \enquote{dog} and the word \enquote{cat} are identical, since they both represent an animal, however a computer would assume that they are completely different things since all the letters in those  words are different. 

    \subsection{Word Embeddings}

    \par The dominant approach to solve this problem is the usage of word embeddings, which is a type of word representation that allows words with similar meaning to have a similar representation by mapping a set of words, or phrases in a vocabulary, to vectors of numerical values. For example, the word “happy” can be represented as a vector of 4 dimensions [0.24, 0.45, 0.11, 0.49] and “sad” as the following vector [0.88, 0.78, 0.45, 0.91]. The reason for this vectors to exist is so that a machine learning algorithm can perform linear algebra operations on numbers (vectors) instead of words \cite{MuratMustafa}. Word embedding methods learn a real-valued vector representation for a predefined fixed size vocabulary from a corpus  of text \cite{Brownlee2017}. A vector representation of a word may be a one-hot encoded vector where 1 stands for the position where the word exists and 0 everywhere else. 
    
    \par As an example, the sentence ``Word Embeddings are Word converted into numbers” can be converted to the following dictionary using the one-hot encoded vector representation : [``Word”,``Embeddings”,``are”,``Converted”,``Word”,``into”,``numbers”]. Using this representation the word ``numbers” in the one-hot encoded vector is [0,0,0,0,0,1] and for the word ``converted” is [0,0,0,1,0,0]. This is considered to be the most simple method to represent words in vector forms \cite{Vidhya2017}. Figure \ref{fig:onehott} showcases the given example.
    
    
    \begin{figure}[H]
        \centering
        \captionsetup{justification=centering}
        \includegraphics[width=0.65\textwidth]{Sections/3StateOfTheArt/3_images/one_hot_encoding.png}
        \caption{Example of text representation by one-hot vector.}   
        \label{fig:onehott}
    \end{figure}
    
    
    



    \section{Static Word Embedding Models}
    \label{sec:static}

    \par This section introduces some common static word embedding models to learn word embeddings from text.


    \par Static word embedding have the fundamental problem which is that they generate the same embedding, in different contexts, for the same word and once learned they do not change it. They map each word type to a single vector, making it harder to deal with the polysemy problem. This is due to the fact that each word has a single vector, regardless of context \cite{Mikolov2013}. Therefore, these models assume that the meaning of any given word is the same across the entire text.
   

    \par As an example, having the following two phrases:

    \begin{itemize}
        \item ``I saw her at the library.”
        \item ``Pass me the saw to cut the log in half.”
    \end{itemize}

    \par In this case, the word ``saw” has two different meanings. In the first phrase the word ``saw” refers to the verb ``see” and for the second phrase it refers to the tool ``saw”. However, for static word embedding models, words only have one single meaning and therefore  the word representation for \enquote{saw} would be the same in both cases.

    Dynamic word embeddings models represent ``saw” differently according to the contexts, while static embedding can not distinguish the semantic difference between two ``saws” and represent with the same vector no matter the context \cite{Wang2020} \cite{Batista2018}.
\newpage
   
        \subsection{Word2Vec}

        One of the most important and most popular models developed in NLP was Word2Vec. Created by Tomas Mikolov, et al. \cite{Mikolov2013} at Google in 2013, Word2Vec is a two-layer neural network that processes text by \enquote{vectorizing} words with the purpose of grouping vectors of similar words together in vectorspace. These similarities are detected by creating vectors that are distributed numerical representations of word features, without human intervention. A vocabulary is outputted from Word2Vec where each item has a vector attached to it. This can be fed into a deep-learning network or queried to detect relationships between words, like similarities. The similarities are measured trough a cosine similarity, having no similarity is expressed as a cosine similarity of 0 since it is 90 degree angle, while a full similarity is expressed as cosine similarity of 1 and it is a 0 degree angle, complete overlap. Sweden is equal to Sweden therefore the cosine similarity is equal to 1, while Norway has a cosine distance of 0.760124 from Sweden \cite{Wiki}.
        
        


        In a regular one-hot encoded vector all words have the same distance between each other, even though their meanings are completely different, this creates a lost in information at the encoding.  However, Word2Vec is capable of learning vectors by understanding the context in which words appear. This results in vectors in which words with similar meanings end up with a similar numerical representation in the vectorspace. Figure \ref{fig:one_vs_word2vec} illustrates this situation. For instance, cats and dogs are more similar than fish and sharks. This extra information is useful for machine learning algorithms \cite{word2vec_explained}.	


        \begin{figure}[H]
            \centering
            \captionsetup{justification=centering}
          
            \begin{subfigure}{0.32\textwidth}
            \includegraphics[width=\textwidth]{Sections/3StateOfTheArt/3_images/one_hot_ex.png} 
            \caption[One-hot encoding resulting vector.]{One-hot encoding resulting vector \cite{word2vec_explained}. }
          
            \end{subfigure}
            \begin{subfigure}{0.32\textwidth}
            \includegraphics[width=\textwidth]{Sections/3StateOfTheArt/3_images/word2vec_encode.png}\hfill
            \caption[Word2Vec encoding resulting vector]{Word2Vec encoding resulting vector \cite{word2vec_explained}. }
            \end{subfigure}
            \caption[One-hot encoding vs Word2Vec encoding.]{One-hot encoding vs Word2Vec encoding.}
            \label{fig:one_vs_word2vec}

          \end{figure}

          Furthermore, using a word offset technique, Word2Vec is capable of performing simple algebraic operations to the word vectors. An example is that the vector(``King”) - vector(``Man”) + vector(``Woman”) results in a vector that is closest to the vector representation of the word ``Queen” \cite{Mikolov2013}. 

        
        Word2Vec is composed of two different models, CBOW (Continuous Bag of words) which predicts a word given the context and Skip-Gram which predicts context given a word \cite{Mikolov2013} \cite{Wiki} as shown in Figure \ref{fig:cbow_skip}.

        

        \begin{figure}[H]
            \centering
            \captionsetup{justification=centering}
            \includegraphics[width=0.8\textwidth]{Sections/3StateOfTheArt/3_images/Cbow_Skip.png}
            \caption[CBOW and Skip-gram models]{CBOW and Skip-gram models \cite{Mikolov2013}.} 
            \label{fig:cbow_skip}
        \end{figure}



        \begin{itemize}
            \item \textbf{Continuous Bag of Words}
        \end{itemize}


       CBOW allows to take a big amount of text and train a neural network to predict a word by inputting the N words at each side. In the given example in Figure \ref{fig:cbow_skip}  N = 2. Using the example given in \cite{Mordecki2017} : “The monkey is eating a banana”, the word “eating” is predicted given that the previous two words are “The” and “monkey” and the next two are “a” and “banana”.

        Using Figure \ref{fig:cbow_skip} again, the inputs of CBOW would be: w(t+2) = “monkey”, w(t+1) = “is”, w(t-1) = “a”, w(t+2) = “banana” and the output (prediction) would be w(t) = “eating” \cite{Mordecki2017}.
        
        
        \begin{itemize}
            \item \textbf{Skip-gram}
        \end{itemize}

        Skip-gram is much identical to CBOW but instead of predicting a word given the context, it predicts the context given a word. 

        Once again recurring to Figure \ref{fig:cbow_skip}, the input of Skip-gram would be: w(t) = “eating” and the outputs would be : w(t+2) = “monkey”, w(t+1) = “is”, w(t-1) = “a”, w(t+2) = “banana” \cite{Mordecki2017}.


        
        \subsection{GloVe}
            \par GloVe stands for Global Vectors for Word Representation and was a new approach created by Pennington et all. in 2014 \cite{Pennington2014} to generate word embeddings with unsupervised learning. Glove main goals are to create word vectors that capture meaning in the vector space and to take advantage of global count statistics instead of using only local information. 
            \par The problem with Word2Vec is that it only takes local information into account, and does not consider global context. This means that the semantics learnt for a given word are only affected by the surrounding words. 
            \par GloVe works by aggregating global word-to-word co-occurrence matrix from a of corpus text. This means that if two words keep appearing together in a corpus of text they either share a linguistic or a semantic similarity. Simply put, similar words will be placed together in the high-dimensional space. Therefore, GloVe can be seen like an extension to the Word2Vec model.

        \subsection{FastText}

        \par FastText, created by Facebook's AI Research (FAIR) lab in 2016, is a fast text classifier based on the skipgram model  used for efficient learning of word representations and sentence classification. Popular models like word2Vec and GloVe  are based on continuous word representations that create vectors directly from words in a sentence while ignoring the morphology of words, this is done by assigning a distinct vector to each word, fastText uses a different approach treating each word as bag of characters n-grams. A vector representation is associated to each character n-gram and words are represented as the sum of these representations. This allows fastText to work with rare words not seen in the training data since the word is broken down into n-grams to get the corresponding embeddings \cite{bojanowski2016enriching}.


        \par Using the word \enquote{where} as an example and n=3, the representation of this word in a fastText model is <wh, whe, her, er, re> and the special sequence <where>. The angular brackets serve as boundary symbols to distinguish the n-gram of a word from the word itself, this means that if the word \enquote{her} was part of the vocabulary it would be represented as <her>, which allows the preservation of the meaning of shorter words and the understanding of suffixes and prefixes.

        

    
    \section{Contextualized Word Embedding Models}
    \label{sec:context}    
        \par Contextualized words embeddings aim at using different embeddings for different word contexts to address the issue of polysemous and the context-dependent nature of words \cite{Batista2018}. Using the example given in  Section \ref{sec:static}, these models would be able to distinguish the different meaning of the word \enquote{saw} given the two different sentences.

        \subsection{Context2vec}
        
            \par Context2Vec is an unsupervised model capable of learning efficiently generic context embedding of wide sentential contexts, using a bidirectional 
            long short-term memory (LSTM) recurrent neural network. 

            A large plain text corpora is utilized in order to learn a neural model capable of embedding  entire  sentential  contexts  and  target words in the same low-dimensional space, which is optimized to reflect inter-dependencies between targets and their entire sentential context as a whole. 
            
            
            In contrast to word2vec that uses context modeling mostly internally and considers the target word embeddings as their main output, the focus of context2vec is the context representation. Context2vec achieves this objective by assigning similar embeddings to sentential contexts and their associated target words \cite{Melamud2016}.

            \begin{figure}[H]
                \centering
                \captionsetup{justification=centering}
                \includegraphics[width=0.38\textwidth]{Sections/3StateOfTheArt/3_images/context2vec_embedding.png}
                \caption[Context2vec’s embedded space and similarity metrics.]{A 2D illustration of context2vec’s embedded space and similarity metrics. Triangles and circles denote sentential context embeddings and target word embeddings, respectively \cite{Melamud2016}.} 
            \end{figure}

            \begin{figure}[H]
                \centering
                \captionsetup{justification=centering}
                \includegraphics[width=0.8\textwidth]{Sections/3StateOfTheArt/3_images/context2vec_predict.png}
                \caption[Context2vec's closest target words]{Closest target words to various sentential contexts, illustrating context2vec’s sensitivity to long range dependencies, and both sides of the target word \cite{Melamud2016}.} 
            \end{figure}
            
        

        \subsection{ELMo}

            \par ELMo (Embeddings from Language Models) is a NLP model with context-aware representation, it understands different meanings for the same word since it takes into account the surrounding words unlike traditional word embedding models such as Word2Vec and GLoVe. In order to achieve this, ELMo attributes an embedding for each word after looking at the entire context in which it is used, instead of using fixed embeddings for each word. Therefore, the same word might have different word vectors under different contexts.
            \par It models both syntax and semantics of word use and how these uses vary across linguistic context. The word vectors are learned through the usage of internal states of a deep bidirectional LSTM algorithm, trained on a large corpus of text. Bidirectional implies that the algorithm takes into account the words before and the words after it in both directions. LSTM (Long Short-Term Memory) is one type of neural network that is able to retain data in memory for long periods of time, allowing it to learn longer-term dependencies.
            This language model can predict both the next word and the previous word and it is a character based model allowing the network to use morphological clues to form robust representations for out-of-vocabulary tokens not presented during training \cite{Peters:2018}.
            \par Figure \ref{fig:diffe} presents an example of the differences between GLOVe that is a non-context aware model and ELMo biLM (bidirectional Language Model) that is context aware.

            
        \begin{figure}[H]
            \centering
            \captionsetup{justification=centering}
            \includegraphics[width=0.85\textwidth]{Sections/3StateOfTheArt/3_images/ELMO.png}
            \caption[Nearest neighbors to \enquote{play} using GLoVe]{Nearest neighbors to \enquote{play} using GLoVe and context embeddings from a biLM \cite{Peters:2018}.} 
            \label{fig:diffe}
        \end{figure}

        \par GLoVe only uses the word \enquote{play} as source, therefore the obtained neighbors for that word are spread across several parts of speech however they all focus on the sports-related sense of the word \enquote{play}. ELMo biLM uses the entire sentence as source, this means that it is able to understand the context of the word. Therefore, in both cases, the biLM is able to disambiguate both the part of speech and word sense in the source sentence \cite{Peters:2018}.
        

       
\section{Available NLP libraries}
    \label{sec:libraries}

    There is a wide array of NLP tools and services available. Knowing their features is important in order to find the most appropriate one for the project at hands. Some might be better for smaller project and others better for experts working with big data projects. Furthermore, NLP libraries solve the problem of requiring superior knowledge of mathematics, machine learning, and linguistics. Using these tools, developers can simplify text processing so that they can concentrate on building machine learning models. 



        \begin{itemize}
            \item\textbf{SpaCy}
        \end{itemize}
      
        
        \par SpaCy is a free, open-source library for advanced natural language processing written in Python and Cython published by Explosion AI. It was designed specifically for production use and to help in the building of applications that process and \enquote{understand} large volumes of text data.  Some use cases for this specific library are to build information extraction or natural language understanding systems, or to pre-process text for deep learning \cite{Spacy2017}. Some of the features that SpaCy offers are: 

        
        \begin{itemize}[leftmargin=1cm]  
            \renewcommand{\labelitemi}{$\textperiodcentered$}
        
            \item - Tokenization: The segmentation of text into words, punctuation, etc
            \item - Part-of-Speech Tagging: The assignment of word types to tokens, like verb, noun, etc
            \item - Similarity: The comparison between different words, phrases or text documents and how similar they are.
            \item - Lemmatization: The assignment of base forms of words.
   
            \renewcommand{\labelitemi}{$\textbullet$}
        \end{itemize}
        
        \newpage
        
        \begin{itemize}
            \item\textbf{Natural Language ToolKit}
        \end{itemize}
      
 

        \par Developed by Steven Bird, Edward Loper and Ewan Klein in the Department of Computer and Information Science at the University of Pennsylvania, NLTK (Natural Language ToolKit) is a suite of open source program modules, tutorials, problem sets and a leading platform for building Python programs to work with human language data. NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora. This library provides easy-to-use interfaces such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning \cite{Loper2002}. 

        \begin{itemize}
            \item\textbf{Stanford Core NLP}
        \end{itemize}
      

        \par Developed at Stanford University, Core NLP is a library written in Java with wrappers for different languages, including Python. This library is fast and some of its components can be integrated to NLTK which boosts efficiency. CoreNLP enables users to derive linguistic annotations for text, including token and sentence boundaries, parts of speech, named entities, numeric and time values, dependency and constituency parses, coreference, sentiment, quote attributions, and relations \cite{Manning2015}.

        \begin{itemize}
            \item\textbf{Gensim}
        \end{itemize}

    

        \par Gensim (\enquote{Generate Similar}) is a Natural Language Processing open-source library for unsupervised topic modeling (a technique to extract the underlying topics from large volumes of text)  and for natural language processing. This python-cython library specializes in finding the semantic similarity between two documents through vector space modeling and topic modeling toolkit. It is capable of building document or word vectors, corpora, performing topic identification, performing document comparison (retrieving semantically similar documents) and analysing plain-text documents for semantic structure. In terms of producing word embedding, gensim allows for the usage of Word2Vec and fastText \cite{rehurek_lrec}.
        
        \begin{itemize}
            \item\textbf{TextBlob}
        \end{itemize}

     

        TextBlob, also a Python library,  offers an API for performing NLP tasks, like part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, language translation, word inflection, parsing, n-grams, and WordNet integration \cite{textblob}.

        \begin{itemize}
            \item\textbf{Flair}
        \end{itemize}
   

        Flair allows the usage of state-of-the-art NLP models for entity recognition (NER), part-of-speech tagging (PoS), sense disambiguation and classification \cite{akbik2018coling}.
        
        \begin{itemize}
            \item\textbf{Polyglot}
        \end{itemize}
       

        This python NLP package supports various multilingual applications and offers the following tasks
        language detection (196 languages), tokenization (165 languages), named entity recognition (40 Languages), part of speech tagging (16 languages), sentiment analysis (136 Languages) \cite{polyglot:2013:ACL-CoNLL}.
    


        
        \section{Final Remarks}
        \label{sec:final_remarks}

        In this thesis the NLP library that was chosen for the development of the text processing phase of the automatic image retrieval system was SpaCy. This decision was based of the fact that there was some previous knowledge thanks to the work developed last year for the ImageCLEF challenge \cite{Ribeiro2019}. In addition, SpaCy also provides state-of-the-art NLP models, many useful features, it is easy to use and it offers a simple and well written documentation which makes it very beginner friendly. 
        
        
